{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6529b172-be86-4e8e-ae8e-61e20c4c2a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "说明文档\n",
    "我们这个文件只针对用来做这一步即 在post-train后 再finetune高质量的英语数据集 我们目前做的是 gemma2 如果情况好的话  我们再post-train后的llama3.3和qwe2.5rlhf中做\n",
    "这个英语的finetune。\n",
    "在这个文件中 我们实现了生成text 翻转 处理label 合并prompt 同时我们没有fold列  order_index其实在这里没那么重要（原因是我们没有合并多个数据集 不会产生重叠）\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5338ba-bbe9-442f-84f2-d7e4b5358b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "### load tokenizer\n",
    "MODEL_NAME = \"/root/autodl-tmp/gemma_9b_it_simpo\"  ## 模型的路径\n",
    "save_name = \"gemma2\"  ##例子 gemma2\n",
    "MAX_LENGTH = 3072\n",
    "MAX_PROMPT_LENGTH = 768\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a858560-c309-47cf-8de5-147e57216b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load template\n",
    "################################# gemma2 template ########################################################\n",
    "if save_name == 'gemma2':\n",
    "    template_1 = (\n",
    "        \"<start_of_turn>user\\n\"\n",
    "        \"Act as an impartial judge and evaluate the quality of responses A and B to the user question. \"\n",
    "        \"Choose the response that better follows the user’s instructions, considering factors such as helpfulness, \"\n",
    "        \"relevance, accuracy, depth, creativity, and level of detail. Be aware that the prompt and responses may be \"\n",
    "        \"incomplete due to input length limitations. The evaluation should not be influenced by position biases, \"\n",
    "        \"presentation order, or response lengths. Do not favor specific assistant names.\\n\"\n",
    "    )\n",
    "    template_2 = \"Question:\"\n",
    "    template_3 = \"\\n\"\n",
    "    template_4 = \"Response A:\"\n",
    "    template_5 = \"\\n\"\n",
    "    template_6 = \"Response B:\"\n",
    "    template_7 = \"<end_of_turn>\\n<start_of_turn>model\"\n",
    "\n",
    "    template_1tokenized = tokenizer(template_1, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_2tokenized = tokenizer(template_2, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_3tokenized = tokenizer(template_3, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_4tokenized = tokenizer(template_4, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_5tokenized = tokenizer(template_5, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_6tokenized = tokenizer(template_6, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_7tokenized = tokenizer(template_7, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "################################# qwen2.5 template ########################################################\n",
    "elif save_name == 'qwen2.5':\n",
    "    template_1 = (\n",
    "        \"<im_start>system\\n\"\n",
    "        \"Act as an impartial judge and evaluate the quality of responses A and B to the user question. \"\n",
    "        \"Choose the response that better follows the user’s instructions, considering factors such as helpfulness, \"\n",
    "        \"relevance, accuracy, depth, creativity, and level of detail. Be aware that the prompt and responses may be \"\n",
    "        \"incomplete due to input length limitations. The evaluation should not be influenced by position biases, \"\n",
    "        \"presentation order, or response lengths. Do not favor specific assistant names.<im_end>\\n\"\n",
    "    )\n",
    "    template_2 = \"<im_start>user\\nQuestion:\"\n",
    "    template_3 = \"\\n\"\n",
    "    template_4 = \"Response A:\"\n",
    "    template_5 = \"\\n\"\n",
    "    template_6 = \"Response B:\"\n",
    "    template_7 = \"<im_end>\\n<|im_start|>assistant\"\n",
    "\n",
    "    template_1tokenized = tokenizer(template_1, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_2tokenized = tokenizer(template_2, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_3tokenized = tokenizer(template_3, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_4tokenized = tokenizer(template_4, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_5tokenized = tokenizer(template_5, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_6tokenized = tokenizer(template_6, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_7tokenized = tokenizer(template_7, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "################################## llama3.3 template ##########################################################################\n",
    "else:\n",
    "    template_1 = (\n",
    "        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "        \"Act as an impartial judge and evaluate the quality of responses A and B to the user question. \"\n",
    "        \"Choose the response that better follows the user’s instructions, considering factors such as helpfulness, \"\n",
    "        \"relevance, accuracy, depth, creativity, and level of detail. Be aware that the prompt and responses may be \"\n",
    "        \"incomplete due to input length limitations. The evaluation should not be influenced by position biases, \"\n",
    "        \"presentation order, or response lengths. Do not favor specific assistant names.\"\n",
    "        \"<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "    )\n",
    "    template_2 = \"Question:\"\n",
    "    template_3 = \"\\n\"\n",
    "    template_4 = \"Response A:\"\n",
    "    template_5 = \"\\n\"\n",
    "    template_6 = \"Response B:\"\n",
    "    template_7 = \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "    template_1tokenized = tokenizer(template_1, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_2tokenized = tokenizer(template_2, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_3tokenized = tokenizer(template_3, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_4tokenized = tokenizer(template_4, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_5tokenized = tokenizer(template_5, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_6tokenized = tokenizer(template_6, add_special_tokens=False)[\"input_ids\"]\n",
    "    template_7tokenized = tokenizer(template_7, add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "######################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "### tokenize data\n",
    "def tokenize_shape(prompt, response_a, response_b, template_1tokenized, template_2tokenized, template_3tokenized, \n",
    "                   template_4tokenized, template_5tokenized, template_6tokenized, template_7tokenized, max_length, max_prompt_length):\n",
    "    p = tokenizer(prompt, add_special_tokens=False, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "    a = tokenizer(response_a, add_special_tokens=False, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "    b = tokenizer(response_b, add_special_tokens=False, truncation=True, max_length=max_length)[\"input_ids\"]\n",
    "\n",
    "    tokenized = {\"input_ids\": [], \"attention_mask\": []}\n",
    "    for _p, _a, _b in zip(p, a, b):  # 同步遍历 prompt, response_a 和 response_b\n",
    "        if len(_p) > max_prompt_length:\n",
    "            _p = _p[-max_prompt_length:]\n",
    "        len_max = len(template_1tokenized) + len(template_2tokenized) + len(template_3tokenized) + \\\n",
    "                  len(template_4tokenized) + len(template_5tokenized) + len(template_6tokenized) + len(template_7tokenized)\n",
    "        rl = (max_length - len(_p) - len_max) // 2\n",
    "        input_ids = [tokenizer.bos_token_id] + template_1tokenized + template_2tokenized + _p + template_3tokenized + \\\n",
    "                    template_4tokenized + _a[-rl:] + template_5tokenized + template_6tokenized + _b[-rl:] + \\\n",
    "                    template_7tokenized + [tokenizer.eos_token_id]\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        tokenized[\"input_ids\"].append(input_ids)\n",
    "        tokenized[\"attention_mask\"].append(attention_mask)\n",
    "    return tokenized\n",
    "\n",
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b,template_1tokenized,template_2tokenized,template_3tokenized,template_4tokenized,template_5tokenized,template_6tokenized,template_7tokenized, max_length,max_prompt_length\n",
    "):\n",
    "    prompt = [ p for p in prompt]\n",
    "    response_a = [ r_a for r_a in response_a]\n",
    "    response_b = [ r_b for r_b in response_b]\n",
    "    tokenized = tokenize_shape(prompt, response_a, response_b,template_1tokenized,template_2tokenized,template_3tokenized,template_4tokenized,template_5tokenized,template_6tokenized,template_7tokenized,max_length,max_prompt_length)\n",
    "    input_ids = tokenized[\"input_ids\"]\n",
    "    attention_mask = tokenized[\"attention_mask\"]\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f5df7-3a97-42df-80fd-db99a4f6562f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('') ##我们要finetune的额外数据\n",
    "\n",
    "def do(row):\n",
    "    if row['winner']=='model_a':\n",
    "        return \"A\"\n",
    "    else:\n",
    "        return \"B\"\n",
    "\n",
    "train['label'] = train.apply(lambda row: do(row), axis=1)\n",
    "print(train['label'].value_counts())\n",
    "\n",
    "train[\"input_ids\"], train[\"attention_mask\"] = tokenize(tokenizer, train[\"prompt\"], train[\"response_a\"], train[\"response_b\"],template_1tokenized,template_2tokenized,template_3tokenized,template_4tokenized,template_5tokenized,template_6tokenized,template_7tokenized,MAX_LENGTH,MAX_PROMPT_LENGTH)\n",
    "train['text'] = train['input_ids'].apply(lambda x: tokenizer.decode(x))\n",
    "\n",
    "\n",
    "\n",
    "train_reverse = train.copy()    \n",
    "\n",
    "def do(row):\n",
    "    if row['winner']=='model_a':\n",
    "        return \"B\"\n",
    "    else:\n",
    "        return \"A\"\n",
    "\n",
    "train_reverse['label'] = train_reverse.apply(lambda row: do(row), axis=1)\n",
    "print(train_reverse['label'].value_counts())\n",
    "\n",
    "train_reverse[\"input_ids\"], train_reverse[\"attention_mask\"] = tokenize(tokenizer, train_reverse[\"prompt\"], train_reverse[\"response_b\"], train_reverse[\"response_a\"],template_1tokenized,template_2tokenized,template_3tokenized,template_4tokenized,template_5tokenized,template_6tokenized,template_7tokenized,MAX_LENGTH,MAX_PROMPT_LENGTH)\n",
    "train_reverse['text'] = train_reverse['input_ids'].apply(lambda x: tokenizer.decode(x))\n",
    "\n",
    "train_reverse['reverse'] = True\n",
    "train['reverse'] = False\n",
    "train = pd.concat([train, train_reverse], axis=0)\n",
    "train['order_index'] = list(range(len(train)))\n",
    "\n",
    "\n",
    "###记得这一步每处理一个数据集 order_index就要改变 2000000 \n",
    "train['order_index'] = train['order_index'] + 2000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4694f0a5-f68a-404b-bbcc-98b0867782d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do(x):\n",
    "    if x == \"B\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "train['label'] = train['label'].apply(lambda x: do(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6acfc3-f675-435c-bbf5-e584a048ffba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7001fae-a267-47f2-8c04-4e93614981d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##这一步是合并相同的prompt\n",
    "grouped_matched_rows = train.groupby('prompt', sort=False).apply(lambda x: x)\n",
    "grouped_matched_rows = grouped_matched_rows.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125657b5-a117-43ca-8b64-ee5243b7755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\".........pkl\", 'wb') as f:\n",
    "    pickle.dump(grouped_matched_rows, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
